extern "C" %{

/**
 * This second example shows how to create a simple jdf that has only one single task.
 *    JDF syntax
 *    parsec_JDFNAME_New()
 *    parsec_context_add_taskpool()
 *    parsec_data_collection_init()
 *
 * Can play with the HelloWorld bounds to show embarissingly parallel algorithm.
 *
 * @version 3.0
 * @email parsec-users@icl.utk.edu
 *
 */

#define gpuErrchk(ans)                        \
	{                                         \
		gpuAssert((ans), __FILE__, __LINE__, true); \
	}
inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort)
{
	if (code != cudaSuccess)
	{
		fprintf(stderr, "GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
		if (abort)
			exit(code);
	}
}

/* Paste code to allocate a matrix in desc if cond_init is true */
#define PASTE_CODE_ALLOCATE_MATRIX(DC, COND, TYPE, INIT_PARAMS)                                     \
    TYPE##_t *DC = (TYPE##_t*)malloc(sizeof(TYPE##_t)*PROBLEM_DC);                                                                        \
    if(COND) {                                                                                      \
        for(int i=0;i<PROBLEM_DC;++i)                                                               \
        {                                                                                           \
            TYPE##_init INIT_PARAMS;                                                                \
            DC[i].grid = parsec_data_allocate((size_t)DC[i].nb_local_tiles *                              \
                                            (size_t)DC[i].bsiz *                                       \
                                            (size_t)parsec_datadist_getsizeoftype(DC[i].mtype));       \
            assert(DC[i].grid != NULL);                                                                \
            parsec_data_collection_set_key((parsec_data_collection_t*)&DC[i], #DC);                    \
        }                                                                                           \
    }

#include <math.h>
#include "parsec.h"
#include "parsec/data_dist/matrix/matrix.h"
#include "parsec/data_dist/matrix/two_dim_rectangle_cyclic.h"
#include "parsec/data_dist/multidimensional_grid.h"

#include "cublas_v2.h"

// LBM defines

#define EPSILON 0.000000001

#define CFL 0.45
#define ALPHA 0.9
#define BETA 0.9


// HPC defines

#define MAX_SUBGRIDS 256

#define PROBLEM_SIZE_X (128)
#define PROBLEM_SIZE_Y (128)
#define PROBLEM_SIZE_Z (128)

#define PROBLEM_D 1
#define PROBLEM_C 1

#define PROBLEM_DC (PROBLEM_D*PROBLEM_C)

#define TILE_SIZE_X (32)
#define TILE_SIZE_Y (32)
#define TILE_SIZE_Z (32)

#define SUBGRID_NUMBER_X ((PROBLEM_SIZE_X)/(TILE_SIZE_X))
#define SUBGRID_NUMBER_Y ((PROBLEM_SIZE_Y)/(TILE_SIZE_Y))
#define SUBGRID_NUMBER_Z ((PROBLEM_SIZE_Z)/(TILE_SIZE_Z))

typedef struct Grid
{
	int size[3];
	int subspaces[3];
	int subgridLogicSizeCompressed[3];
	int subgridLogicSize[3];
	int subgridTrueSize[3];
	size_t subgridCompressedSize[MAX_SUBGRIDS];
	size_t subgridTempSize[MAX_SUBGRIDS];
	int cellsPerSubgrid;
	int subgridsNumber;
	double **subgridsCPU;
	double **subgrids[2];
	int currentSubgrid;
	int overlap;
	double mass[MAX_SUBGRIDS];
	double physicalMinCoords[3];
	double physicalSize[3];
} Grid;

Grid newGrid(void)
{
	Grid grid;

	grid.size[0] = PROBLEM_SIZE_X;
	grid.size[1] = PROBLEM_SIZE_Y;
	grid.size[2] = PROBLEM_SIZE_Z;
	grid.subspaces[0] = 2;
	grid.subspaces[1] = 2;
	grid.subspaces[2] = 2;
	grid.subgridLogicSize[0] = PROBLEM_SIZE_X / grid.subspaces[0] + 1;
	grid.subgridLogicSize[1] = PROBLEM_SIZE_Y / grid.subspaces[1] + 1;
	grid.subgridLogicSize[2] = PROBLEM_SIZE_Z / grid.subspaces[2] + 1;
	grid.overlap = 1;

	grid.currentSubgrid = 0;

	grid.physicalMinCoords[0] = -1;
	grid.physicalMinCoords[1] = -1;
	grid.physicalMinCoords[2] = -1;
	grid.physicalSize[0] = 2;
	grid.physicalSize[1] = 2;
	grid.physicalSize[2] = 2;

	grid.subgridLogicSizeCompressed[0] = grid.subgridLogicSize[0];
	grid.subgridLogicSizeCompressed[1] = grid.subgridLogicSize[1];
	grid.subgridLogicSizeCompressed[2] = grid.subgridLogicSize[2];
	grid.subgridTrueSize[0] = grid.subgridLogicSize[0] + 2 * grid.overlap;
	grid.subgridTrueSize[1] = grid.subgridLogicSize[1] + 2 * grid.overlap;
	grid.subgridTrueSize[2] = grid.subgridLogicSize[2] + 2 * grid.overlap;
	grid.cellsPerSubgrid = (grid.subgridTrueSize[0]) * (grid.subgridTrueSize[1]) * (grid.subgridTrueSize[2]);
	grid.subgridsNumber = grid.subspaces[0] * grid.subspaces[1] * grid.subspaces[2];

	grid.subgridsCPU = (double **)malloc(sizeof(double *) * grid.subgridsNumber);
	grid.subgrids[0] = (double **)malloc(sizeof(double *) * grid.subgridsNumber);
	grid.subgrids[1] = (double **)malloc(sizeof(double *) * grid.subgridsNumber);

	for (int i = 0; i < grid.subgridsNumber; ++i)
	{
		// init to a random value
		grid.subgridCompressedSize[i] = 0;
		grid.subgridTempSize[i] = 0;

		// malloc the subgrids on the CPU
		grid.subgridsCPU[i] = (double *)malloc(grid.cellsPerSubgrid * sizeof(double));

		//gpuErrchk(cudaMalloc(&grid.subgrids[0][i], grid.cellsPerSubgrid * sizeof(double)));
		//gpuErrchk(cudaMalloc(&grid.subgrids[1][i], grid.cellsPerSubgrid * sizeof(double)));

		// fill with zeros
		for (int j = 0; j < grid.cellsPerSubgrid; ++j)
		{
			grid.subgridsCPU[i][j] = 0;
		}
	}

	return grid;
}


typedef cublasStatus_t (*cublas_dgemm_v2_t) ( cublasHandle_t handle,
                            cublasOperation_t transa, cublasOperation_t transb,
                            int m, int n, int k,
                            const double *alpha,
                            const double *A, int lda,
                            const double *B, int ldb,
                            const double *beta,
                            double       *C, int ldc);


#if defined(PARSEC_HAVE_CUDA)
static void destruct_cublas_handle(void *p)
{
    cublasHandle_t handle = (cublasHandle_t)p;
    cublasStatus_t status;
    if(NULL != handle) {
        status = cublasDestroy(handle);
        assert(status == CUBLAS_STATUS_SUCCESS);
        (void)status;
    }
}

static void *create_cublas_handle(void *obj, void *p)
{
    cublasHandle_t handle;
    cublasStatus_t status;
    parsec_cuda_exec_stream_t *stream = (parsec_cuda_exec_stream_t *)obj;
    (void)p;
    /* No need to call cudaSetDevice, as this has been done by PaRSEC before calling the task body */
    status = cublasCreate(&handle);
    assert(CUBLAS_STATUS_SUCCESS == status);
    status = cublasSetStream(handle, stream->cuda_stream);
    assert(CUBLAS_STATUS_SUCCESS == status);
    (void)status;
    return (void*)handle;
}
#endif

static void destroy_cublas_handle(void *_h, void *_n)
{
#if defined(PARSEC_HAVE_CUDA)
    cublasHandle_t cublas_handle = (cublasHandle_t)_h;
    cublasDestroy_v2(cublas_handle);
#endif
    (void)_n;
    (void)_h;
}


%}


descGridDC  [ type="parsec_multidimensional_grid_t*" ]


rank   [ type="int" ]
nodes   [ type="int" ]
subgrid_number_x   [ type="int" ] // row number
subgrid_number_y   [ type="int" ] // column number
subgrid_number_z   [ type="int" ]
tile_size_x   [ type="int" ]
tile_size_y   [ type="int" ]
tile_size_z   [ type="int" ]
conservatives_number   [ type="int" ]
directions_number   [ type="int" ]
number_of_steps   [ type="int" ]
CuHI              [type = "parsec_info_id_t"]


FillGrid(x, y, z, c, d)

x = 0 .. subgrid_number_x-1
y = 0 .. subgrid_number_y-1
z = 0 .. subgrid_number_z-1
c = 0 .. conservatives_number-1
d = 0 .. directions_number-1

: descGridDC(x, y, z, c, d)

/*
RW INITIAL_GRID <- descGridDC(x, y, z, c, d)
    -> (c==0 && d==0) ? GRID_CD_0_0 LBM(x, y, z)
*/
RW INITIAL_GRID <- descGridDC(x, y, z, c, d)
    -> (c==0 && d==0) ? GRID_CD_0_0 LBM_SUBGRID(x, y, z, c, d, 0)

BODY
    double *grid = INITIAL_GRID;

    for(int i=0;i<TILE_SIZE_X*TILE_SIZE_Y*TILE_SIZE_Z;++i)
    {
        grid[i] = 1;
    }
END


LBM_SUBGRID(x, y, z, c, d, s)

x = 0 .. subgrid_number_x-1
y = 0 .. subgrid_number_y-1
z = 0 .. subgrid_number_z-1
c = 0 .. conservatives_number-1
d = 0 .. directions_number-1
s = 0 .. number_of_steps-1

: descGridDC(x, y, z, c, d)

//READ GRID <- (s==0) ? INITIAL_GRID FillGrid(x, y, z, c, d)

READ GRID_CD_0_0 <- (c==0 && d==0 && s>0) ? GRID_CD_0_0 LBM_STEP(x, y, z, s-1)
    <- (c==0 && d==0 && s==0) ? INITIAL_GRID FillGrid(x, y, z, c, d)
    -> (c==0 && d==0) ? GRID_CD_0_0 LBM_STEP(x, y, z, s)

    //-> (s<number_of_steps-1) ? GRID LBM_SUBGRID(x, y, z, c, d, s+1) : descGridDC(x, y, z, c, d)

BODY [type=CUDA]
/*
    double *grid = GRID;

    printf("[Process %d] kernel LBM (%d, %d, %d) is called (subgrid=%p)\n", rank, x, y, z, grid);
*/
END


LBM_STEP(x, y, z, s)

x = 0 .. subgrid_number_x-1
y = 0 .. subgrid_number_y-1
z = 0 .. subgrid_number_z-1
s = 0 .. number_of_steps-1

: descGridDC(x, y, z, 0, 0)

RW GRID_CD_0_0 <- GRID_CD_0_0 LBM_SUBGRID(x, y, z, 0, 0, s)
    -> (s<number_of_steps-1) ? GRID_CD_0_0 LBM_SUBGRID(x, y, z, 0, 0, s+1) : descGridDC(x, y, z, 0, 0)

BODY [type=CUDA]
    double *subgrid[PROBLEM_C][PROBLEM_D];
    subgrid[0][0] = GRID_CD_0_0;

    printf("[Process %d] kernel LBM (%d, %d, %d) is called (subgrid=%p)\n", rank, x, y, z, &subgrid[0][0]);
END

extern "C" %{

int main(int argc, char *argv[])
{
    parsec_context_t* parsec;
    int rc;
    int rank, world;
    parsec_LBM_taskpool_t *tp;
    //int mycounter;

    parsec_arena_datatype_t adt;
    parsec_datatype_t otype;

#if defined(PARSEC_HAVE_MPI)
    {
        int provided;
        MPI_Init_thread(&argc, &argv, MPI_THREAD_SERIALIZED, &provided);
    }
    MPI_Comm_size(MPI_COMM_WORLD, &world);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
#else
    world = 1;
    rank = 0;
#endif

    // LBM parameters
    Grid grid = newGrid();

    double delta_x = grid.physicalSize[0] / (double)grid.size[0];
    double dt = CFL * delta_x / (ALPHA > BETA ? ALPHA : BETA);

    double tmax = 1;
    int number_of_steps = (int)((tmax-EPSILON) / dt) + 1;

    parsec = parsec_init(-1, &argc, &argv);

    int nodes = world;

    #if defined(PARSEC_HAVE_CUDA)
    parsec_info_id_t CuHI = parsec_info_register(&parsec_per_stream_infos, "CUBLAS::HANDLE",
                                                 destroy_cublas_handle, NULL,
                                                 create_cublas_handle, NULL,
                                                 NULL);
    assert(CuHI != -1);
#else
    int CuHI = -1;
#endif

    PASTE_CODE_ALLOCATE_MATRIX(descGridDC, true, parsec_multidimensional_grid, (
                               &descGridDC[i],
                               PARSEC_MATRIX_DOUBLE,
                               nodes, rank,
                               5,
                               SUBGRID_NUMBER_X, SUBGRID_NUMBER_Y, SUBGRID_NUMBER_Z, PROBLEM_C, PROBLEM_D,
                               TILE_SIZE_X, TILE_SIZE_Y, TILE_SIZE_Z, 1, 1,
                               1, 1, 1, 1, 1)
                            );

    parsec_translate_matrix_type(PARSEC_MATRIX_DOUBLE, &otype);
    parsec_add2arena_rect(&adt, otype,
                                 TILE_SIZE_X, TILE_SIZE_Y*TILE_SIZE_Z*PROBLEM_C*PROBLEM_D, TILE_SIZE_X);

    tp = (parsec_LBM_taskpool_t*)parsec_LBM_new(
                                descGridDC,
                                rank, world,
                                SUBGRID_NUMBER_X, SUBGRID_NUMBER_Y, SUBGRID_NUMBER_Z,
                                TILE_SIZE_X, TILE_SIZE_Y, TILE_SIZE_Z, PROBLEM_C, PROBLEM_D,
                                number_of_steps,
                                CuHI);

    assert( NULL != tp );
    tp->arenas_datatypes[PARSEC_LBM_DEFAULT_ADT_IDX] = adt;
    PARSEC_OBJ_RETAIN(adt.arena);

    rc = parsec_context_add_taskpool( parsec, (parsec_taskpool_t*)tp );
    PARSEC_CHECK_ERROR(rc, "parsec_context_add_taskpool");
    rc = parsec_context_start(parsec);
    PARSEC_CHECK_ERROR(rc, "parsec_context_start");
    rc = parsec_context_wait(parsec);
    PARSEC_CHECK_ERROR(rc, "parsec_context_wait");

printf("final : %f\n", ((double*)(descGridDC[0].grid))[0]);


    parsec_taskpool_free(&tp->super);

    for(int i=0;i<PROBLEM_DC;++i)
    {
        parsec_data_free(descGridDC[i].grid);
        parsec_grid_destroy(&descGridDC[i]);
    }

    parsec_fini(&parsec);
#if defined(PARSEC_HAVE_MPI)
    MPI_Finalize();
#endif

    return 0;
}

%}