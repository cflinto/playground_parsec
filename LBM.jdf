extern "C" %{

/**
 * This second example shows how to create a simple jdf that has only one single task.
 *    JDF syntax
 *    parsec_JDFNAME_New()
 *    parsec_context_add_taskpool()
 *    parsec_data_collection_init()
 *
 * Can play with the HelloWorld bounds to show embarissingly parallel algorithm.
 *
 * @version 3.0
 * @email parsec-users@icl.utk.edu
 *
 */

#define gpuErrchk(ans)                        \
	{                                         \
		gpuAssert((ans), __FILE__, __LINE__, true); \
	}
inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort)
{
	if (code != cudaSuccess)
	{
		fprintf(stderr, "GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
		if (abort)
			exit(code);
	}
}

#include <math.h>
#include "parsec.h"
#include "parsec/data_dist/matrix/matrix.h"
#include "parsec/data_dist/matrix/two_dim_rectangle_cyclic.h"
#include "parsec/data_dist/multidimensional_grid.h"

#include "cublas_v2.h"

// LBM defines

#define EPSILON 0.000000001

#define CFL 0.45
#define ALPHA 0.9
#define BETA 0.9


// HPC defines

#define DIMENSIONS_NUMBER 3


#define MAX_SUBGRIDS 256

#define PROBLEM_SIZE_X (64)
#define PROBLEM_SIZE_Y (64)
#define PROBLEM_SIZE_Z (64)

typedef struct Grid
{
    // Difference between the logical size, the true size, and the owned size in 1D:
    // Subgrid:
    // OOOAAAAAAAAASOOO

    // Terminology:
    // O = overlap (= ghost cells = halo)
    // A = owned
    // S = shared
    // true size = O + A + S
    // logical size = A + S
    // owned size = A
    // In this case (overlap=3) and with a basic 2-direction scheme (left/right):
    // We can perform 3 consecutive steps (because we have 3 ghost cells)
    // After the 3 steps, A and S are set to the right values, but O is not.
    // The overlap must, therefore, be synchronized with the neighbors at this point in time.

    // Conceptually, the grid is a 5D array: (x, y, z, c, d) of size (size[0], size[1], size[2], conservativesNumber, directionsNumber)
	int size[DIMENSIONS_NUMBER];
    int conservativesNumber;
    int directionsNumber;

	int subgridNumber[DIMENSIONS_NUMBER];
	int subgridLogicalSizeCompressed[DIMENSIONS_NUMBER];
	int subgridOwnedSize[DIMENSIONS_NUMBER]; // cells owned uniquely by this subgrid
	int subgridLogicalSize[DIMENSIONS_NUMBER]; // cells that need computation on this subgrid (includes cells that share the same computation)
	int subgridTrueSize[DIMENSIONS_NUMBER]; // all the cells of the subgrid (including ghost cells = halo = overlap)
    int sharedLayers[DIMENSIONS_NUMBER][2]; // number of values that are shared with the neighbor [dim][dir] (shared meaning that the values are computed twice and not exchanged)
	int overlapSize[DIMENSIONS_NUMBER]; // depth of the overlap on each dimension


    // Conceptually, the grid is a 5D array: (x, y, z, c, d)

	size_t subgridCompressedSize[MAX_SUBGRIDS];
	size_t subgridTempSize[MAX_SUBGRIDS];
	int cellsPerSubgrid;
	int subgridsNumber;
	int currentSubgrid;

	double mass[MAX_SUBGRIDS];

    // physical coords of the problem, used by the physical model
	double physicalMinCoords[DIMENSIONS_NUMBER];
	double physicalSize[DIMENSIONS_NUMBER];

    parsec_multidimensional_grid_t desc;
} Grid;

Grid newGrid(int rank, int nodes)
{
	Grid grid;

	grid.size[0] = PROBLEM_SIZE_X;
	grid.size[1] = PROBLEM_SIZE_Y;
	grid.size[2] = PROBLEM_SIZE_Z;
    grid.conservativesNumber = 3;
    grid.directionsNumber = 2;

	grid.subgridNumber[0] = 2;
	grid.subgridNumber[1] = 2;
	grid.subgridNumber[2] = 2;
	grid.subgridOwnedSize[0] = grid.size[0] / grid.subgridNumber[0];
	grid.subgridOwnedSize[1] = grid.size[1] / grid.subgridNumber[1];
	grid.subgridOwnedSize[2] = grid.size[2] / grid.subgridNumber[2];

    grid.sharedLayers[0][0] = 0;
    grid.sharedLayers[0][1] = 1;
    grid.sharedLayers[1][0] = 0;
    grid.sharedLayers[1][1] = 1;
    grid.sharedLayers[2][0] = 0;
    grid.sharedLayers[2][1] = 1;

    for(int d=0;d<DIMENSIONS_NUMBER;++d)
    {
        grid.subgridLogicalSize[d] = grid.subgridOwnedSize[d] + grid.sharedLayers[d][0] + grid.sharedLayers[d][1];
    }


	grid.overlapSize[0] = 1;
	grid.overlapSize[1] = 3;
	grid.overlapSize[2] = 4;

	grid.currentSubgrid = 0;

	grid.physicalMinCoords[0] = -1;
	grid.physicalMinCoords[1] = -1;
	grid.physicalMinCoords[2] = -1;
	grid.physicalSize[0] = 2;
	grid.physicalSize[1] = 2;
	grid.physicalSize[2] = 2;

	grid.subgridLogicalSizeCompressed[0] = grid.subgridLogicalSize[0];
	grid.subgridLogicalSizeCompressed[1] = grid.subgridLogicalSize[1];
	grid.subgridLogicalSizeCompressed[2] = grid.subgridLogicalSize[2];
	grid.subgridTrueSize[0] = grid.subgridLogicalSize[0] + 2 * grid.overlapSize[0];
	grid.subgridTrueSize[1] = grid.subgridLogicalSize[1] + 2 * grid.overlapSize[1];
	grid.subgridTrueSize[2] = grid.subgridLogicalSize[2] + 2 * grid.overlapSize[2];
	grid.cellsPerSubgrid = (grid.subgridTrueSize[0]) * (grid.subgridTrueSize[1]) * (grid.subgridTrueSize[2]);
	grid.subgridsNumber = grid.subgridNumber[0] * grid.subgridNumber[1] * grid.subgridNumber[2];

    int numberOfSharedLayers = 1; // TODO !
    for(int d=0;d<DIMENSIONS_NUMBER;++d)
    {
        assert(grid.size[d] % grid.subgridOwnedSize[d] == 0);
        assert(grid.subgridNumber[d]*grid.subgridOwnedSize[d] == grid.size[d]);
        assert(grid.subgridOwnedSize[d] <= grid.subgridLogicalSize[d]);
        assert(grid.subgridLogicalSize[d] <= grid.subgridTrueSize[d]);
    }

	for (int i = 0; i < grid.subgridsNumber; ++i)
	{
		// init to a random value
		grid.subgridCompressedSize[i] = 0;
		grid.subgridTempSize[i] = 0;
	}

    //grid.desc = (parsec_multidimensional_grid_t*)malloc(sizeof(parsec_multidimensional_grid_t)*grid.conservativesNumber*grid.directionsNumber);
    //assert(grid.desc != NULL);

    //for(int i=0;i<grid.conservativesNumber*grid.directionsNumber;++i)
    int i=0;
    {
        parsec_multidimensional_grid_init(&grid.desc,
                               PARSEC_MATRIX_DOUBLE,
                               nodes, rank,
                               5, 3,
                               grid.subgridNumber[0], grid.subgridNumber[1], grid.subgridNumber[2], grid.conservativesNumber, grid.directionsNumber,
                               grid.subgridTrueSize[0], grid.subgridTrueSize[1], grid.subgridTrueSize[2],
                               1, 1, 1, 1, 1);
        grid.desc.grid = parsec_data_allocate((size_t)grid.desc.nb_local_tiles *
                                        (size_t)grid.desc.bsiz *
                                        (size_t)parsec_datadist_getsizeoftype(grid.desc.mtype));
        assert(grid.desc.grid != NULL);

        parsec_data_collection_set_key((parsec_data_collection_t*)&grid.desc, "grid.desc[");
    }

	return grid;
}


typedef cublasStatus_t (*cublas_dgemm_v2_t) ( cublasHandle_t handle,
                            cublasOperation_t transa, cublasOperation_t transb,
                            int m, int n, int k,
                            const double *alpha,
                            const double *A, int lda,
                            const double *B, int ldb,
                            const double *beta,
                            double       *C, int ldc);


#if defined(PARSEC_HAVE_CUDA)
static void destruct_cublas_handle(void *p)
{
    cublasHandle_t handle = (cublasHandle_t)p;
    cublasStatus_t status;
    if(NULL != handle) {
        status = cublasDestroy(handle);
        assert(status == CUBLAS_STATUS_SUCCESS);
        (void)status;
    }
}

static void *create_cublas_handle(void *obj, void *p)
{
    cublasHandle_t handle;
    cublasStatus_t status;
    parsec_cuda_exec_stream_t *stream = (parsec_cuda_exec_stream_t *)obj;
    (void)p;
    /* No need to call cudaSetDevice, as this has been done by PaRSEC before calling the task body */
    status = cublasCreate(&handle);
    assert(CUBLAS_STATUS_SUCCESS == status);
    status = cublasSetStream(handle, stream->cuda_stream);
    assert(CUBLAS_STATUS_SUCCESS == status);
    (void)status;
    return (void*)handle;
}
#endif

static void destroy_cublas_handle(void *_h, void *_n)
{
#if defined(PARSEC_HAVE_CUDA)
    cublasHandle_t cublas_handle = (cublasHandle_t)_h;
    cublasDestroy_v2(cublas_handle);
#endif
    (void)_n;
    (void)_h;
}

//int cd;

%}


descGridDC  [ type="parsec_multidimensional_grid_t*" ]
gridParameters       [ type="Grid" ]


rank   [ type="int" ]
nodes   [ type="int" ]
subgrid_number_x   [ type="int" ] // row number
subgrid_number_y   [ type="int" ] // column number
subgrid_number_z   [ type="int" ]
tile_size_x   [ type="int" ]
tile_size_y   [ type="int" ]
tile_size_z   [ type="int" ]
conservatives_number   [ type="int" ]
directions_number   [ type="int" ]
overlap_x   [ type="int" ]
overlap_y   [ type="int" ]
overlap_z   [ type="int" ]
number_of_steps   [ type="int" ]
CuHI              [type = "parsec_info_id_t"]


FillGrid(x, y, z, c, d)

x = 0 .. subgrid_number_x-1
y = 0 .. subgrid_number_y-1
z = 0 .. subgrid_number_z-1
c = 0 .. conservatives_number-1
d = 0 .. directions_number-1

: descGridDC(x, y, z, c, d)

/*
RW INITIAL_GRID <- descGridDC(x, y, z, c, d)
    -> (c==0 && d==0) ? GRID_CD_0_0 LBM_STEP(x, y, z, 0)
    -> (c==1 && d==0) ? GRID_CD_1_0 LBM_STEP(x, y, z, 0)
    -> (c==2 && d==0) ? GRID_CD_2_0 LBM_STEP(x, y, z, 0)
    -> (c==0 && d==1) ? GRID_CD_0_1 LBM_STEP(x, y, z, 0)
    -> (c==1 && d==1) ? GRID_CD_1_1 LBM_STEP(x, y, z, 0)
    -> (c==2 && d==1) ? GRID_CD_2_1 LBM_STEP(x, y, z, 0)
*/

// RW TEST_PARAMETRIZED_FLOW[cd] [cd=0..conservatives_number*directions_number-1]
//     <- descGridDC(x, y, z,cd%conservatives_number, cd/conservatives_number)

RW INITIAL_GRID <- descGridDC(x, y, z, c, d)
    -> GRID_CD[cd = (c+d*conservatives_number)] LBM_STEP(x, y, z, 0)
    
// CTL TEST_BROADCAST_3 <- (c==0 && d==0) ? TEST_BROADCAST_3 LBM_STEP(x, y, z, 0)

// CTL TEST_BROADCAST_1 -> (z==0 && c==0 && d==0) ? [z_iterator_2=0..subgrid_number_z-1] TEST_BROADCAST_1 LBM_STEP(x, y, z_iterator_2, 0) : [z_iterator_2=0..subgrid_number_z-1] TEST_BROADCAST_1 LBM_STEP(x, y, z_iterator_2, 0)

// CTL TEST_BROADCAST_2 -> [z_iterator=0..subgrid_number_z-1] (z==0 && c==0 && d==0) ? TEST_BROADCAST_2 LBM_STEP(x, y, z_iterator, 0)

// CTL TEST_BROADCAST_4 -> [z_iterator=0..subgrid_number_z-1] (z==0 && c==0 && d==0) ? TEST_BROADCAST_4 LBM_STEP(x, y, z_iterator, 0)

// CTL TEST_BROADCAST_5 -> [z_iterator=0..subgrid_number_z-1] (z==0 && c==0 && d==0) ? TEST_BROADCAST_5 LBM_STEP(x, y, z_iterator, 0)

BODY
    double *grid = INITIAL_GRID;

    // double *test_subgrid_0 = TEST_PARAMETRIZED_FLOW[0];
    // double *test_subgrid_1 = TEST_PARAMETRIZED_FLOW[1];
    // double *test_subgrid_2 = TEST_PARAMETRIZED_FLOW[2];
    // double *test_subgrid_3 = TEST_PARAMETRIZED_FLOW[3];
    // double *test_subgrid_4 = TEST_PARAMETRIZED_FLOW[4];
    // double *test_subgrid_5 = TEST_PARAMETRIZED_FLOW[5];

    for(int i=0;i<tile_size_x*tile_size_y*tile_size_z;++i)
    {
        grid[i] = c+d*conservatives_number;
        grid[i] = 0;
    }

    printf("FillGrid: %d %d %d %d %d (grid=%p)\n", x, y, z, c, d, grid);
    // printf("FillGrid: %d %d %d %d %d (grid=%p)   (test_subgrid_0=%p, test_subgrid_1=%p, test_subgrid_2=%p, test_subgrid_3=%p, test_subgrid_4=%p, test_subgrid_5=%p)\n",
    //         x, y, z, c, d, grid, test_subgrid_0, test_subgrid_1, test_subgrid_2, test_subgrid_3, test_subgrid_4, test_subgrid_5);
END


LBM_STEP(x, y, z, s)

x = 0 .. subgrid_number_x-1
y = 0 .. subgrid_number_y-1
z = 0 .. subgrid_number_z-1
s = 0 .. number_of_steps-1

: descGridDC(x, y, z, 0, 0)

/*
RW GRID_CD[cd] [cd=0..conservatives_number*directions_number-1]
    <- [p=0..3] X TASK(n, p)
*/
/*
RW GRID_CD[cd] [cd=0..conservatives_number*directions_number-1]
    <- (s==0) ? INITIAL_GRID FillGrid(x, y, z, cd%conservatives_number, cd/directions_number)
        : GRID_TO Exchange(x, y, z, s-1, cd%conservatives_number, cd/directions_number, 2, 1)
*/



 //       : GRID_TO Exchange(x, y, z, s-1, %{ return cd%conservatives_number; %}, cd/directions_number, 2, 1)

// do a dump_task_class
// + asseerts ...

// CTL TEST_BROADCAST_1 <- TEST_BROADCAST_1 FillGrid(x, y, 0, 0, 0)

// CTL TEST_BROADCAST_2 <- TEST_BROADCAST_2 FillGrid(x, y, 0, 0, 0)

// CTL TEST_BROADCAST_3 -> TEST_BROADCAST_3 FillGrid(x, y, z, 0, 0)

RW GRID_CD[cd = 0..conservatives_number*directions_number-1]
    <- (s==0) ? INITIAL_GRID FillGrid(x, y, z, cd%conservatives_number, cd/conservatives_number)
        : GRID_TO Exchange(x, y, z, s-1, cd%conservatives_number, cd/conservatives_number, 2, 1)

    -> (s==number_of_steps-1) ? FINAL_GRID WriteBack(x, y, z, cd%conservatives_number, cd/conservatives_number)
        : GRID_TO Exchange(x, y, z, s, cd%conservatives_number, cd/conservatives_number, 0, 0)

// CTL TEST_BROADCAST_4 <- TEST_BROADCAST_4 FillGrid(x, y, 0, 0, 0)

// CTL TEST_BROADCAST_5 <- TEST_BROADCAST_5 FillGrid(x, y, 0, 0, 0)

// CTL TEST_CTL[cd] [cd = 0..conservatives_number*directions_number-1]
//     <- (s>0) ? TEST_CTL[cd] LBM_STEP(x, y, z, s-1)
//     -> (s<number_of_steps-1) ? TEST_CTL[cd] LBM_STEP(x, y, z, s+1)


// RW GRID_CD_0_0 <- (s==0) ? INITIAL_GRID FillGrid(x, y, z, 0, 0) : GRID_TO Exchange(x, y, z, s-1, 0, 0, 2, 1)
//     -> (s==number_of_steps-1) ? FINAL_GRID WriteBack(x, y, z, 0, 0) : GRID_TO Exchange(x, y, z, s, 0, 0, 0, 0)

// RW GRID_CD_1_0 <- (s==0) ? INITIAL_GRID FillGrid(x, y, z, 1, 0) : GRID_TO Exchange(x, y, z, s-1, 1, 0, 2, 1)
//     -> (s==number_of_steps-1) ? FINAL_GRID WriteBack(x, y, z, 1, 0) : GRID_TO Exchange(x, y, z, s, 1, 0, 0, 0)

// RW GRID_CD_2_0 <- (s==0) ? INITIAL_GRID FillGrid(x, y, z, 2, 0) : GRID_TO Exchange(x, y, z, s-1, 2, 0, 2, 1)
//     -> (s==number_of_steps-1) ? FINAL_GRID WriteBack(x, y, z, 2, 0) : GRID_TO Exchange(x, y, z, s, 2, 0, 0, 0)

// RW GRID_CD_0_1 <- (s==0) ? INITIAL_GRID FillGrid(x, y, z, 0, 1) : GRID_TO Exchange(x, y, z, s-1, 0, 1, 2, 1)
//     -> (s==number_of_steps-1) ? FINAL_GRID WriteBack(x, y, z, 0, 1) : GRID_TO Exchange(x, y, z, s, 0, 1, 0, 0)

// RW GRID_CD_1_1 <- (s==0) ? INITIAL_GRID FillGrid(x, y, z, 1, 1) : GRID_TO Exchange(x, y, z, s-1, 1, 1, 2, 1)
//     -> (s==number_of_steps-1) ? FINAL_GRID WriteBack(x, y, z, 1, 1) : GRID_TO Exchange(x, y, z, s, 1, 1, 0, 0)

// RW GRID_CD_2_1 <- (s==0) ? INITIAL_GRID FillGrid(x, y, z, 2, 1) : GRID_TO Exchange(x, y, z, s-1, 2, 1, 2, 1)
//     -> (s==number_of_steps-1) ? FINAL_GRID WriteBack(x, y, z, 2, 1) : GRID_TO Exchange(x, y, z, s, 2, 1, 0, 0)

// CTL X <- (s!=0) ? X LBM_STEP(x, y, z, s-1)
//     -> (s!=number_of_steps-1) ? X LBM_STEP(x, y, z, s+1)


// BODY [type=CUDA]
//     double *subgrid[gridParameters.conservativesNumber][gridParameters.directionsNumber];
//     subgrid[0][0] = GRID_CD[0];
//     subgrid[1][0] = GRID_CD[1];
//     subgrid[2][0] = GRID_CD[2];
//     subgrid[0][1] = GRID_CD[3];
//     subgrid[1][1] = GRID_CD[4];
//     subgrid[2][1] = GRID_CD[5];

//     printf("[Process %d] kernel LBM_STEP GPU (%d %d %d %d) grid|0][0]=%p, grid[1][0]=%p, grid[2][0]=%p, grid[0][1]=%p, grid[1][1]=%p, grid[2][1]=%p\n",
//                 rank, x, y, z, s, subgrid[0][0], subgrid[1][0], subgrid[2][0], subgrid[0][1], subgrid[1][1], subgrid[2][1]);

//     /*for(int i=0;i<tile_size_x*tile_size_y*tile_size_z;++i)
//     {
//         for(int cd=0;cd<gridParameters.conservativesNumber*gridParameters.directionsNumber;++cd)
//         {
//             double *mat = GRID_CD[cd];
//             mat[i] += 1;
//         }
//     }*/
// END

BODY
    double *subgrid[gridParameters.conservativesNumber][gridParameters.directionsNumber];
    subgrid[0][0] = GRID_CD[0];
    subgrid[1][0] = GRID_CD[1];
    subgrid[2][0] = GRID_CD[2];
    subgrid[0][1] = GRID_CD[3];
    subgrid[1][1] = GRID_CD[4];
    subgrid[2][1] = GRID_CD[5];


    printf("[Process %d] kernel LBM_STEP CPU (%d %d %d %d) grid|0][0]=%p, grid[1][0]=%p, grid[2][0]=%p, grid[0][1]=%p, grid[1][1]=%p, grid[2][1]=%p\n",
                rank, x, y, z, s, subgrid[0][0], subgrid[1][0], subgrid[2][0], subgrid[0][1], subgrid[1][1], subgrid[2][1]);

    printf("[Process %d] kernel LBM_STEP CPU (%d %d %d %d) first value of each matrix: %f %f %f %f %f %f\n",
                rank, x, y, z, s, subgrid[0][0][0], subgrid[1][0][0], subgrid[2][0][0], subgrid[0][1][0], subgrid[1][1][0], subgrid[2][1][0]);

    for(int i=0;i<tile_size_x*tile_size_y*tile_size_z;++i)
    {
        for(int cd=0;cd<gridParameters.conservativesNumber*gridParameters.directionsNumber;++cd)
        {
            double *mat = GRID_CD[cd];
            mat[i] += 1;
        }
    }

END



Exchange(x, y, z, s, conservative, direction, dimension, side)

x = 0 .. subgrid_number_x-1
y = 0 .. subgrid_number_y-1
z = 0 .. subgrid_number_z-1
s = 0 .. number_of_steps-2
conservative = 0 .. conservatives_number-1
direction = 0 .. directions_number-1
dimension = 0 .. 3-1
side = 0 .. 1
//side = %{ return conservative; %}

// name of the paper : Extending the PTG paradigm to run efficient high-order stencil codes

: descGridDC(x, y, z, conservative, direction)

//cd = conservative + direction * gridParameters.conservativesNumber;

RW GRID_TO <- (dimension==0 && side==0) ? [other_var=2] GRID_CD[cd = conservative+direction*conservatives_number] LBM_STEP(x, y, z, s)
            : GRID_TO Exchange(x, y, z, s, conservative, direction, ((side==0)?(dimension-1):(dimension)), (side+1)%2)
        -> (dimension==2 && side==1) ? [other_var=3] GRID_CD[cd = conservative+direction*conservatives_number] LBM_STEP(x, y, z, s+1)
            : GRID_TO Exchange(x, y, z, s, conservative, direction, ((side==1)?(dimension+1):(dimension)), (side+1)%2)

/*
    <- (conservative==0 && direction==0 && dimension==0 && side==0) ? GRID_CD_0_0 LBM_STEP(x, y, z, s)
    <- (conservative==0 && direction==0 && !(dimension==0 && side==0)) ? GRID_TO Exchange(x, y, z, s, conservative, direction, ((side==0)?(dimension-1):(dimension)), (side+1)%2)
    <- (conservative==1 && direction==0 && dimension==0 && side==0) ? GRID_CD_1_0 LBM_STEP(x, y, z, s)
    <- (conservative==1 && direction==0 && !(dimension==0 && side==0)) ? GRID_TO Exchange(x, y, z, s, conservative, direction, ((side==0)?(dimension-1):(dimension)), (side+1)%2)
    <- (conservative==2 && direction==0 && dimension==0 && side==0) ? GRID_CD_2_0 LBM_STEP(x, y, z, s)
    <- (conservative==2 && direction==0 && !(dimension==0 && side==0)) ? GRID_TO Exchange(x, y, z, s, conservative, direction, ((side==0)?(dimension-1):(dimension)), (side+1)%2)
    <- (conservative==0 && direction==1 && dimension==0 && side==0) ? GRID_CD_0_1 LBM_STEP(x, y, z, s)
    <- (conservative==0 && direction==1 && !(dimension==0 && side==0)) ? GRID_TO Exchange(x, y, z, s, conservative, direction, ((side==0)?(dimension-1):(dimension)), (side+1)%2)
    <- (conservative==1 && direction==1 && dimension==0 && side==0) ? GRID_CD_1_1 LBM_STEP(x, y, z, s)
    <- (conservative==1 && direction==1 && !(dimension==0 && side==0)) ? GRID_TO Exchange(x, y, z, s, conservative, direction, ((side==0)?(dimension-1):(dimension)), (side+1)%2)
    <- (conservative==2 && direction==1 && dimension==0 && side==0) ? GRID_CD_2_1 LBM_STEP(x, y, z, s)
    <- (conservative==2 && direction==1 && !(dimension==0 && side==0)) ? GRID_TO Exchange(x, y, z, s, conservative, direction, ((side==0)?(dimension-1):(dimension)), (side+1)%2)

    -> (conservative==0 && direction==0 && dimension==2 && side==1) ? GRID_CD_0_0 LBM_STEP(x, y, z, s+1)
    -> (conservative==0 && direction==0 && !(dimension==2 && side==1)) ? GRID_TO Exchange(x, y, z, s, conservative, direction, ((side==1)?(dimension+1):(dimension)), (side+1)%2)
    -> (conservative==1 && direction==0 && dimension==2 && side==1) ? GRID_CD_1_0 LBM_STEP(x, y, z, s+1)
    -> (conservative==1 && direction==0 && !(dimension==2 && side==1)) ? GRID_TO Exchange(x, y, z, s, conservative, direction, ((side==1)?(dimension+1):(dimension)), (side+1)%2)
    -> (conservative==2 && direction==0 && dimension==2 && side==1) ? GRID_CD_2_0 LBM_STEP(x, y, z, s+1)
    -> (conservative==2 && direction==0 && !(dimension==2 && side==1)) ? GRID_TO Exchange(x, y, z, s, conservative, direction, ((side==1)?(dimension+1):(dimension)), (side+1)%2)
    -> (conservative==0 && direction==1 && dimension==2 && side==1) ? GRID_CD_0_1 LBM_STEP(x, y, z, s+1)
    -> (conservative==0 && direction==1 && !(dimension==2 && side==1)) ? GRID_TO Exchange(x, y, z, s, conservative, direction, ((side==1)?(dimension+1):(dimension)), (side+1)%2)
    -> (conservative==1 && direction==1 && dimension==2 && side==1) ? GRID_CD_1_1 LBM_STEP(x, y, z, s+1)
    -> (conservative==1 && direction==1 && !(dimension==2 && side==1)) ? GRID_TO Exchange(x, y, z, s, conservative, direction, ((side==1)?(dimension+1):(dimension)), (side+1)%2)
    -> (conservative==2 && direction==1 && dimension==2 && side==1) ? GRID_CD_2_1 LBM_STEP(x, y, z, s+1)
    -> (conservative==2 && direction==1 && !(dimension==2 && side==1)) ? GRID_TO Exchange(x, y, z, s, conservative, direction, ((side==1)?(dimension+1):(dimension)), (side+1)%2)
*/

BODY [type=CUDA]
    printf("[Process %d] kernel Exchange (%d %d %d %d %d %d %d %d) grid_to=%p\n",
                rank, x, y, z, s, conservative, direction, dimension, side, GRID_TO);
END

WriteBack(x, y, z, c, d)

x = 0 .. subgrid_number_x-1
y = 0 .. subgrid_number_y-1
z = 0 .. subgrid_number_z-1
c = 0 .. conservatives_number-1
d = 0 .. directions_number-1

: descGridDC(x, y, z, c, d)

RW FINAL_GRID <- GRID_CD[cd = c+d*conservatives_number] LBM_STEP(x, y, z, number_of_steps-1)
    -> descGridDC(x, y, z, c, d)

/*
RW FINAL_GRID[p] [p=0..3] <- (c==0 && d==0) ? GRID_CD_0_0 LBM_STEP(x, y, z, number_of_steps-1)
    <- (c==1 && d==0) ? GRID_CD_1_0 LBM_STEP(x, y, z, number_of_steps-1)
    <- (c==2 && d==0) ? GRID_CD_2_0 LBM_STEP(x, y, z, number_of_steps-1)
    <- (c==0 && d==1) ? GRID_CD_0_1 LBM_STEP(x, y, z, number_of_steps-1)
    <- (c==1 && d==1) ? GRID_CD_1_1 LBM_STEP(x, y, z, number_of_steps-1)
    <- (c==2 && d==1) ? GRID_CD_2_1 LBM_STEP(x, y, z, number_of_steps-1)
*/

BODY
    double *mat = (double*)(FINAL_GRID);

    printf("[Process %d] kernel WRITE_BACK (%d %d %d %d %d)\n", rank, x, y, z, c, d);
    printf("[Process %d] kernel WRITE_BACK (%d %d %d %d %d) first value: %f\n", rank, x, y, z, c, d, mat[0]);
END

extern "C" %{

int main(int argc, char *argv[])
{
    parsec_context_t* parsec;
    int rc;
    int rank, world;
    parsec_LBM_taskpool_t *tp;
    //int mycounter;

    parsec_arena_datatype_t adt;
    parsec_datatype_t otype;

#if defined(PARSEC_HAVE_MPI)
    {
        int provided;
        MPI_Init_thread(&argc, &argv, MPI_THREAD_SERIALIZED, &provided);
    }
    MPI_Comm_size(MPI_COMM_WORLD, &world);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
#else
    world = 1;
    rank = 0;
#endif

    // LBM parameters
    Grid grid = newGrid(rank, world);

    double delta_x = grid.physicalSize[0] / (double)grid.size[0];
    double dt = CFL * delta_x / (ALPHA > BETA ? ALPHA : BETA);

    double tmax = 1;
    int number_of_steps = (int)((tmax-EPSILON) / dt) + 1;
    number_of_steps = 16;

    parsec = parsec_init(-1, &argc, &argv);

    int nodes = world;

    #if defined(PARSEC_HAVE_CUDA)
    parsec_info_id_t CuHI = parsec_info_register(&parsec_per_stream_infos, "CUBLAS::HANDLE",
                                                 destroy_cublas_handle, NULL,
                                                 create_cublas_handle, NULL,
                                                 NULL);
    assert(CuHI != -1);
#else
    int CuHI = -1;
#endif

    parsec_translate_matrix_type(PARSEC_MATRIX_DOUBLE, &otype);
    parsec_add2arena_rect(&adt, otype,
                                 grid.subgridTrueSize[0], grid.subgridTrueSize[1]*grid.subgridTrueSize[2]*grid.conservativesNumber*grid.directionsNumber, grid.subgridTrueSize[0]);

    tp = (parsec_LBM_taskpool_t*)parsec_LBM_new(
                                &grid.desc, grid,
                                rank, world,
                                grid.subgridNumber[0], grid.subgridNumber[1], grid.subgridNumber[2],
                                grid.subgridTrueSize[0], grid.subgridTrueSize[1], grid.subgridTrueSize[2], grid.conservativesNumber, grid.directionsNumber,
                                grid.overlapSize[0], grid.overlapSize[1], grid.overlapSize[2],
                                number_of_steps,
                                CuHI);

    assert( NULL != tp );
    tp->arenas_datatypes[PARSEC_LBM_DEFAULT_ADT_IDX] = adt;
    PARSEC_OBJ_RETAIN(adt.arena);

    rc = parsec_context_add_taskpool( parsec, (parsec_taskpool_t*)tp );
    PARSEC_CHECK_ERROR(rc, "parsec_context_add_taskpool");
    rc = parsec_context_start(parsec);
    PARSEC_CHECK_ERROR(rc, "parsec_context_start");
    rc = parsec_context_wait(parsec);
    PARSEC_CHECK_ERROR(rc, "parsec_context_wait");

    for(int cd=0; cd<grid.conservativesNumber*grid.directionsNumber; cd++) {
        double *mat = &(((double*)grid.desc.grid)[cd*grid.subgridNumber[0]*grid.subgridNumber[1]*grid.subgridNumber[2]]);
        printf("for cd=%d, final : %f\n", cd, *mat);
    }


    parsec_taskpool_free(&tp->super);

    //for(int i=0;i<grid.conservativesNumber*grid.directionsNumber;++i)
    int i=0;
    {
        parsec_data_free(grid.desc.grid);
        parsec_grid_destroy(&grid.desc);
    }

    parsec_fini(&parsec);
#if defined(PARSEC_HAVE_MPI)
    MPI_Finalize();
#endif

    return 0;
}

%}