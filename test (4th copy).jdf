extern "C" %{

/**
 * This second example shows how to create a simple jdf that has only one single task.
 *    JDF syntax
 *    parsec_JDFNAME_New()
 *    parsec_context_add_taskpool()
 *    parsec_data_collection_init()
 *
 * Can play with the HelloWorld bounds to show embarissingly parallel algorithm.
 *
 * @version 3.0
 * @email parsec-users@icl.utk.edu
 *
 */

/* Paste code to allocate a matrix in desc if cond_init is true */
#define PASTE_CODE_ALLOCATE_MATRIX(DC, COND, TYPE, INIT_PARAMS)      \
    TYPE##_t DC;                                                     \
    if(COND) {                                                          \
        TYPE##_init INIT_PARAMS;                                        \
        DC.mat = parsec_data_allocate((size_t)DC.super.nb_local_tiles * \
                                        (size_t)DC.super.bsiz *      \
                                        (size_t)parsec_datadist_getsizeoftype(DC.super.mtype)); \
        parsec_data_collection_set_key((parsec_data_collection_t*)&DC, #DC);          \
    }

#include <math.h>
#include "parsec.h"
#include "parsec/data_dist/matrix/matrix.h"
#include "parsec/data_dist/matrix/two_dim_rectangle_cyclic.h"

%}

descA  [ type="parsec_matrix_block_cyclic_t*" ]
descB  [ type="parsec_matrix_block_cyclic_t*" ]
descC  [ type="parsec_matrix_block_cyclic_t*" ]
rank   [ type="int" ]
nodes   [ type="int" ]
matrix_size_m   [ type="int" ] // row number
matrix_size_n   [ type="int" ] // column number
matrix_size_k   [ type="int" ]

FillA(m, k)

m = 0 .. matrix_size_m-1
k = 0 .. matrix_size_k-1

: descA(m, k)

WRITE A -> A GEMM(m, 0 .. matrix_size_n-1, k)

BODY
    double *matA = A;
printf("%x\n", matA);
    //matA[0] = 3.14159265;
END

FillB(k, n)

n = 0 .. matrix_size_n-1
k = 0 .. matrix_size_k-1

: descB(k, n)

WRITE B -> B GEMM(0 .. matrix_size_m-1, n, k)

BODY
    double *matB = B;
printf("%x\n", matB);
    //matB[0] = 3.14159265;
END

GEMM(m, n, k)

m = 0 .. matrix_size_m-1
n = 0 .. matrix_size_n-1
k = 0 .. matrix_size_k-1

: descA(m, k)

READ A <- A FillA(m, k)
READ B <- B FillB(k, n)

BODY
    printf("process %d is at coordinates %d, %d\n", rank, m, k);
END

extern "C" %{

static uint32_t
rank_of(parsec_data_collection_t *desc, ...)
{
    (void)desc;
    return 0;
}

static int32_t
vpid_of(parsec_data_collection_t *desc, ...)
{
    (void)desc;
    return 0;
}

static parsec_data_t *ddata_A = NULL;

static parsec_data_t *
data_of_A(parsec_data_collection_t *desc, ...)
{
    (void)desc;
    return ddata_A;
}

static parsec_data_t *ddata_B = NULL;

static parsec_data_t *
data_of_B(parsec_data_collection_t *desc, ...)
{
    (void)desc;
    return ddata_B;
}

static parsec_data_t *ddata_C = NULL;

static parsec_data_t *
data_of_C(parsec_data_collection_t *desc, ...)
{
    (void)desc;
    return ddata_C;
}

static parsec_data_key_t
data_key(parsec_data_collection_t *desc, ...)
{
    int k;
    va_list ap;
    (void)desc;
    va_start(ap, desc);
    k = va_arg(ap, int);
    va_end(ap);
    return (uint64_t)k;
}

#define MATRIX_M 4 // M rows
#define MATRIX_N 5 // N columns
#define MATRIX_K 6 // depth
#define TILE_SIZE 8

int main(int argc, char *argv[])
{
    parsec_context_t* parsec;
    int rc;
    int rank, world;
    parsec_taskpool_t *tp;
    parsec_data_collection_t data_A, data_B, data_C;
    //int mycounter;

#if defined(PARSEC_HAVE_MPI)
    {
        int provided;
        MPI_Init_thread(&argc, &argv, MPI_THREAD_SERIALIZED, &provided);
    }
    MPI_Comm_size(MPI_COMM_WORLD, &world);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
#else
    world = 1;
    rank = 0;
#endif

    parsec = parsec_init(-1, &argc, &argv);

    parsec_data_collection_init(&data_A, world, rank);
    data_A.rank_of = rank_of;
    data_A.vpid_of = vpid_of;
    data_A.data_key = data_key;
    data_A.data_of = data_of_A;

    parsec_data_collection_init(&data_B, world, rank);
    data_B.rank_of = rank_of;
    data_B.vpid_of = vpid_of;
    data_B.data_key = data_key;
    data_B.data_of = data_of_B;

    parsec_data_collection_init(&data_C, world, rank);
    data_C.rank_of = rank_of;
    data_C.vpid_of = vpid_of;
    data_C.data_key = data_key;
    data_C.data_of = data_of_C;

    //mycounter = 300 + rank;
    int test_data = 0;

    int nodes = world;

    int KP = sqrt(nodes);
    int KQ = nodes/KP;

    PASTE_CODE_ALLOCATE_MATRIX(descA, true, parsec_matrix_block_cyclic, (
                               &descA,
                               PARSEC_MATRIX_DOUBLE,
                               PARSEC_MATRIX_TILE,
                               rank,
                               TILE_SIZE,      TILE_SIZE,   /* Tile size */
                               MATRIX_M*TILE_SIZE,    MATRIX_K*TILE_SIZE,   /* Global matrix size (what is stored)*/
                               0,           0,    /* Staring point in the global matrix */
                               MATRIX_M,    MATRIX_K,    /* Submatrix size (the one concerned by the computation */
                               KP,          KQ,    /* process process grid*/
                               1,           1,   /* k-cyclicity */
                               0,           0)   /* starting point on the process grid*/
                            );

    PASTE_CODE_ALLOCATE_MATRIX(descB, true, parsec_matrix_block_cyclic, (
                               &descB,
                               PARSEC_MATRIX_DOUBLE,
                               PARSEC_MATRIX_TILE,
                               rank,
                               TILE_SIZE,      TILE_SIZE,   /* Tile size */
                               MATRIX_K*TILE_SIZE,    MATRIX_N*TILE_SIZE,   /* Global matrix size (what is stored)*/
                               0,           0,    /* Staring point in the global matrix */
                               MATRIX_K,    MATRIX_N,    /* Submatrix size (the one concerned by the computation */
                               KP,          KQ,    /* process process grid*/
                               1,           1,   /* k-cyclicity */
                               0,           0)   /* starting point on the process grid*/
                            );

    PASTE_CODE_ALLOCATE_MATRIX(descC, true, parsec_matrix_block_cyclic, (
                               &descC,
                               PARSEC_MATRIX_DOUBLE,
                               PARSEC_MATRIX_TILE,
                               rank,
                               TILE_SIZE,      TILE_SIZE,   /* Tile size */
                               MATRIX_M*TILE_SIZE,    MATRIX_N*TILE_SIZE,   /* Global matrix size (what is stored)*/
                               0,           0,    /* Staring point in the global matrix */
                               MATRIX_M,    MATRIX_N,    /* Submatrix size (the one concerned by the computation */
                               KP,          KQ,    /* process process grid*/
                               1,           1,   /* k-cyclicity */
                               0,           0)   /* starting point on the process grid*/
                            );

    tp = (parsec_taskpool_t*)parsec_test_new(&descA, &descB, &descC, rank, world, MATRIX_M, MATRIX_N, MATRIX_K);

    rc = parsec_context_add_taskpool( parsec, tp );
    PARSEC_CHECK_ERROR(rc, "parsec_context_add_taskpool");
    rc = parsec_context_start(parsec);
    PARSEC_CHECK_ERROR(rc, "parsec_context_start");
    rc = parsec_context_wait(parsec);
    PARSEC_CHECK_ERROR(rc, "parsec_context_wait");

    parsec_taskpool_free(tp);

    parsec_data_free(descA.mat);
    parsec_tiled_matrix_destroy( (parsec_tiled_matrix_t*)&descA);

    parsec_data_free(descB.mat);
    parsec_tiled_matrix_destroy( (parsec_tiled_matrix_t*)&descB);

    parsec_data_free(descC.mat);
    parsec_tiled_matrix_destroy( (parsec_tiled_matrix_t*)&descC);

    parsec_fini(&parsec);
#if defined(PARSEC_HAVE_MPI)
    MPI_Finalize();
#endif

    return 0;
}

%}